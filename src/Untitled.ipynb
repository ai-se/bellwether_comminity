{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import SMOTE\n",
    "import feature_selector\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/old_datasets/Jureczko/ant/ant-1.3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = feature_selector.featureSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46, 0.56, 0.63, 0.68, 0.72, 0.73]\n",
      "4\n",
      "['rfc', 'loc', 'bug']\n"
     ]
    }
   ],
   "source": [
    "df_new,_selected,selected = fs.cfs_bfs(df)\n",
    "print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset = []\n",
    "for i in range(10):\n",
    "    df_new,_selected,selected = fs.cfs_bfs(df)\n",
    "    subset.append(_selected)\n",
    "    print(selected)\n",
    "subset_df = pd.DataFrame(subset, columns = ['$wmc', '$dit', '$noc', '$cbo', '$rfc', '$lcom', '$ca', '$ce', '$npm',\n",
    "       '$lcom3', '$loc', '$dam', '$moa', '$mfa', '$cam', '$ic', '$cbm', '$amc',\n",
    "       '$max_cc', '$avg_cc'])\n",
    "subset_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.bug.values\n",
    "X = df.drop(labels = ['bug'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = []\n",
    "kf = StratifiedKFold(n_splits = 10)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_df = X_train\n",
    "    train_df['bug'] = y_train\n",
    "    test_df = X_test\n",
    "    test_df['bug'] = y_test\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    # using CFS implemented\n",
    "    train_df_new,_selected,selected = fs.cfs_bfs(df)\n",
    "    subset.append(_selected)\n",
    "    train_y = train_df_new.bug.values\n",
    "    train_X = train_df_new.drop(labels = ['bug'],axis = 1)\n",
    "    test_df_new = test_df[selected]\n",
    "    test_y = test_df_new.bug.values\n",
    "    test_X = test_df_new.drop(labels = ['bug'],axis = 1)\n",
    "    clf = LogisticRegression(penalty='l1',class_weight = 'balanced')\n",
    "    clf.fit(train_X,train_y)\n",
    "    predicted = clf.predict(test_X)\n",
    "    print(\"Using Python CFS selected features\")\n",
    "    print(round(metrics.f1_score(test_y, predicted,average='weighted'),2))\n",
    "    \n",
    "    #using Weka CFS\n",
    "    selected = ['wmc','cbo','rfc','lcom','loc','dam','cam','bug']\n",
    "    #selected = ['ce','cbo','bug']\n",
    "    train_df_new = train_df[selected]\n",
    "    train_y = train_df_new.bug.values\n",
    "    train_X = train_df_new.drop(labels = ['bug'],axis = 1)\n",
    "    test_df_new = test_df[selected]\n",
    "    test_y = test_df_new.bug.values\n",
    "    test_X = test_df_new.drop(labels = ['bug'],axis = 1)\n",
    "    clf = LogisticRegression(penalty='l1',class_weight = 'balanced')\n",
    "    clf.fit(train_X,train_y)\n",
    "    predicted = clf.predict(test_X)\n",
    "    print(\"Using Weka CFS selected features\")\n",
    "    print(round(metrics.f1_score(test_y, predicted,average='weighted'),2))\n",
    "subset_df = pd.DataFrame(subset, columns = ['$wmc', '$dit', '$noc', '$cbo', '$rfc', '$lcom', '$ca', '$ce', '$npm',\n",
    "       '$lcom3', '$loc', '$dam', '$moa', '$mfa', '$cam', '$ic', '$cbm', '$amc',\n",
    "       '$max_cc', '$avg_cc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = []\n",
    "kf = StratifiedKFold(n_splits = 10)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    train_df = X_train\n",
    "    train_df['bug'] = y_train\n",
    "    test_df = X_test\n",
    "    test_df['bug'] = y_test\n",
    "    df_new = train_df[['wmc','cbo','rfc','lcom','loc','dam','cam','bug']]\n",
    "    train_y = df_new.bug.values\n",
    "    train_X = df_new.drop(labels = ['bug'],axis = 1)\n",
    "    test_df = test_df[['wmc','cbo','rfc','lcom','loc','dam','cam','bug']]\n",
    "    test_y = test_df.bug.values\n",
    "    test_X = test_df.drop(labels = ['bug'],axis = 1)\n",
    "    clf = LogisticRegression(penalty='l1',class_weight = 'balanced')\n",
    "    clf.fit(train_X,train_y)\n",
    "    predicted = clf.predict(test_X)\n",
    "    print(classification_report(test_y, predicted))\n",
    "subset_df = pd.DataFrame(subset, columns = ['$wmc', '$dit', '$noc', '$cbo', '$rfc', '$lcom', '$ca', '$ce', '$npm',\n",
    "       '$lcom3', '$loc', '$dam', '$moa', '$mfa', '$cam', '$ic', '$cbm', '$amc',\n",
    "       '$max_cc', '$avg_cc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def merit_calculation(X, y):\n",
    "    \"\"\"\n",
    "    This function calculates the merit of X given class labels y, where\n",
    "    merits = (k * rcf)/sqrt(k+k*(k-1)*rff)\n",
    "    rcf = (1/k)*sum(su(fi,y)) for all fi in X\n",
    "    rff = (1/(k*(k-1)))*sum(su(fi,fj)) for all fi and fj in X\n",
    "\n",
    "    Input\n",
    "    ----------\n",
    "    X: {numpy array}, shape (n_samples, n_features)\n",
    "        input data\n",
    "    y: {numpy array}, shape (n_samples,)\n",
    "        input class labels\n",
    "\n",
    "    Output\n",
    "    ----------\n",
    "    merits: {float}\n",
    "        merit of a feature subset X\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    rff = 0\n",
    "    rcf = 0\n",
    "    for i in range(n_features):\n",
    "        fi = X[:, i]\n",
    "        rcf += su_calculation(fi, y)\n",
    "        for j in range(n_features):\n",
    "            if j > i:\n",
    "                fj = X[:, j]\n",
    "                rff += su_calculation(fi, fj)\n",
    "    rff *= 2\n",
    "    merits = rcf / np.sqrt(n_features + rff)\n",
    "    return merits\n",
    "\n",
    "\n",
    "def cfs(X, y):\n",
    "    \"\"\"\n",
    "    This function uses a correlation based heuristic to evaluate the worth of features which is called CFS\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: {numpy array}, shape (n_samples, n_features)\n",
    "        input data\n",
    "    y: {numpy array}, shape (n_samples,)\n",
    "        input class labels\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    F: {numpy array}\n",
    "        index of selected features\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    Zhao, Zheng et al. \"Advancing Feature Selection Research - ASU Feature Selection Repository\" 2010.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    F = []\n",
    "    # M stores the merit values\n",
    "    M = []\n",
    "    while True:\n",
    "        merit = -100000000000\n",
    "        idx = -1\n",
    "        for i in range(n_features):\n",
    "            if i not in F:\n",
    "                F.append(i)\n",
    "                # calculate the merit of current selected features\n",
    "                t = merit_calculation(X[:, F], y)\n",
    "                print(F,t)\n",
    "                if t > merit:\n",
    "                    merit = t\n",
    "                    idx = i\n",
    "                F.pop()\n",
    "        F.append(idx)\n",
    "        M.append(merit)\n",
    "        if len(M) > 5:\n",
    "            if M[len(M)-1] <= M[len(M)-2]:\n",
    "                if M[len(M)-2] <= M[len(M)-3]:\n",
    "                    if M[len(M)-3] <= M[len(M)-4]:\n",
    "                        if M[len(M)-4] <= M[len(M)-5]:\n",
    "                            break\n",
    "    return np.array(F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Greg Ver Steeg (http://www.isi.edu/~gregv/npeet.html)\n",
    "\n",
    "import scipy.spatial as ss\n",
    "from scipy.special import digamma\n",
    "from math import log\n",
    "import numpy.random as nr\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# continuous estimators\n",
    "\n",
    "def entropy(x, k=3, base=2):\n",
    "    \"\"\"\n",
    "    The classic K-L k-nearest neighbor continuous entropy estimator x should be a list of vectors,\n",
    "    e.g. x = [[1.3],[3.7],[5.1],[2.4]] if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "\n",
    "    assert k <= len(x)-1, \"Set k smaller than num. samples - 1\"\n",
    "    d = len(x[0])\n",
    "    N = len(x)\n",
    "    intens = 1e-10  # small noise to break degeneracy, see doc.\n",
    "    x = [list(p + intens * nr.rand(len(x[0]))) for p in x]\n",
    "    tree = ss.cKDTree(x)\n",
    "    nn = [tree.query(point, k+1, p=float('inf'))[0][k] for point in x]\n",
    "    const = digamma(N)-digamma(k) + d*log(2)\n",
    "    return (const + d*np.mean(map(log, nn)))/log(base)\n",
    "\n",
    "\n",
    "def mi(x, y, k=3, base=2):\n",
    "    \"\"\"\n",
    "    Mutual information of x and y; x, y should be a list of vectors, e.g. x = [[1.3],[3.7],[5.1],[2.4]]\n",
    "    if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(x) == len(y), \"Lists should have same length\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    intens = 1e-10  # small noise to break degeneracy, see doc.\n",
    "    x = [list(p + intens * nr.rand(len(x[0]))) for p in x]\n",
    "    y = [list(p + intens * nr.rand(len(y[0]))) for p in y]\n",
    "    points = zip2(x, y)\n",
    "    # Find nearest neighbors in joint space, p=inf means max-norm\n",
    "    tree = ss.cKDTree(points)\n",
    "    dvec = [tree.query(point, k+1, p=float('inf'))[0][k] for point in points]\n",
    "    a, b, c, d = avgdigamma(x, dvec), avgdigamma(y, dvec), digamma(k), digamma(len(x))\n",
    "    return (-a-b+c+d)/log(base)\n",
    "\n",
    "\n",
    "def cmi(x, y, z, k=3, base=2):\n",
    "    \"\"\"\n",
    "    Mutual information of x and y, conditioned on z; x, y, z should be a list of vectors, e.g. x = [[1.3],[3.7],[5.1],[2.4]]\n",
    "    if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(x) == len(y), \"Lists should have same length\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    intens = 1e-10  # small noise to break degeneracy, see doc.\n",
    "    x = [list(p + intens * nr.rand(len(x[0]))) for p in x]\n",
    "    y = [list(p + intens * nr.rand(len(y[0]))) for p in y]\n",
    "    z = [list(p + intens * nr.rand(len(z[0]))) for p in z]\n",
    "    points = zip2(x, y, z)\n",
    "    # Find nearest neighbors in joint space, p=inf means max-norm\n",
    "    tree = ss.cKDTree(points)\n",
    "    dvec = [tree.query(point, k+1, p=float('inf'))[0][k] for point in points]\n",
    "    a, b, c, d = avgdigamma(zip2(x, z), dvec), avgdigamma(zip2(y, z), dvec), avgdigamma(z, dvec), digamma(k)\n",
    "    return (-a-b+c+d)/log(base)\n",
    "\n",
    "\n",
    "def kldiv(x, xp, k=3, base=2):\n",
    "    \"\"\"\n",
    "    KL Divergence between p and q for x~p(x), xp~q(x); x, xp should be a list of vectors, e.g. x = [[1.3],[3.7],[5.1],[2.4]]\n",
    "    if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    assert k <= len(xp) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    assert len(x[0]) == len(xp[0]), \"Two distributions must have same dim.\"\n",
    "    d = len(x[0])\n",
    "    n = len(x)\n",
    "    m = len(xp)\n",
    "    const = log(m) - log(n-1)\n",
    "    tree = ss.cKDTree(x)\n",
    "    treep = ss.cKDTree(xp)\n",
    "    nn = [tree.query(point, k+1, p=float('inf'))[0][k] for point in x]\n",
    "    nnp = [treep.query(point, k, p=float('inf'))[0][k-1] for point in x]\n",
    "    return (const + d*np.mean(map(log, nnp))-d*np.mean(map(log, nn)))/log(base)\n",
    "\n",
    "\n",
    "# Discrete estimators\n",
    "def entropyd(sx, base=2):\n",
    "    \"\"\"\n",
    "    Discrete entropy estimator given a list of samples which can be any hashable object\n",
    "    \"\"\"\n",
    "\n",
    "    return entropyfromprobs(hist(sx), base=base)\n",
    "\n",
    "\n",
    "def midd(x, y):\n",
    "    \"\"\"\n",
    "    Discrete mutual information estimator given a list of samples which can be any hashable object\n",
    "    \"\"\"\n",
    "\n",
    "    return -entropyd(list(zip(x, y)))+entropyd(x)+entropyd(y)\n",
    "\n",
    "\n",
    "def cmidd(x, y, z):\n",
    "    \"\"\"\n",
    "    Discrete mutual information estimator given a list of samples which can be any hashable object\n",
    "    \"\"\"\n",
    "\n",
    "    return entropyd(list(zip(y, z)))+entropyd(list(zip(x, z)))-entropyd(list(zip(x, y, z)))-entropyd(z)\n",
    "\n",
    "\n",
    "def hist(sx):\n",
    "    # Histogram from list of samples\n",
    "    d = dict()\n",
    "    for s in sx:\n",
    "        d[s] = d.get(s, 0) + 1\n",
    "    return map(lambda z: float(z)/len(sx), d.values())\n",
    "\n",
    "\n",
    "def entropyfromprobs(probs, base=2):\n",
    "    # Turn a normalized list of probabilities of discrete outcomes into entropy (base 2)\n",
    "    return -sum(map(elog, probs))/log(base)\n",
    "\n",
    "\n",
    "def elog(x):\n",
    "    # for entropy, 0 log 0 = 0. but we get an error for putting log 0\n",
    "    if x <= 0. or x >= 1.:\n",
    "        return 0\n",
    "    else:\n",
    "        return x*log(x)\n",
    "\n",
    "\n",
    "# Mixed estimators\n",
    "def micd(x, y, k=3, base=2, warning=True):\n",
    "    \"\"\" If x is continuous and y is discrete, compute mutual information\n",
    "    \"\"\"\n",
    "\n",
    "    overallentropy = entropy(x, k, base)\n",
    "    n = len(y)\n",
    "    word_dict = dict()\n",
    "    for sample in y:\n",
    "        word_dict[sample] = word_dict.get(sample, 0) + 1./n\n",
    "    yvals = list(set(word_dict.keys()))\n",
    "\n",
    "    mi = overallentropy\n",
    "    for yval in yvals:\n",
    "        xgiveny = [x[i] for i in range(n) if y[i] == yval]\n",
    "        if k <= len(xgiveny) - 1:\n",
    "            mi -= word_dict[yval]*entropy(xgiveny, k, base)\n",
    "        else:\n",
    "            if warning:\n",
    "                print(\"Warning, after conditioning, on y={0} insufficient data. Assuming maximal entropy in this case.\".format(yval))\n",
    "            mi -= word_dict[yval]*overallentropy\n",
    "    return mi  # units already applied\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def vectorize(scalarlist):\n",
    "    \"\"\"\n",
    "    Turn a list of scalars into a list of one-d vectors\n",
    "    \"\"\"\n",
    "\n",
    "    return [(x,) for x in scalarlist]\n",
    "\n",
    "\n",
    "def shuffle_test(measure, x, y, z=False, ns=200, ci=0.95, **kwargs):\n",
    "    \"\"\"\n",
    "    Shuffle test\n",
    "    Repeatedly shuffle the x-values and then estimate measure(x,y,[z]).\n",
    "    Returns the mean and conf. interval ('ci=0.95' default) over 'ns' runs, 'measure' could me mi,cmi,\n",
    "    e.g. Keyword arguments can be passed. Mutual information and CMI should have a mean near zero.\n",
    "    \"\"\"\n",
    "\n",
    "    xp = x[:]   # A copy that we can shuffle\n",
    "    outputs = []\n",
    "    for i in range(ns):\n",
    "        random.shuffle(xp)\n",
    "        if z:\n",
    "            outputs.append(measure(xp, y, z, **kwargs))\n",
    "        else:\n",
    "            outputs.append(measure(xp, y, **kwargs))\n",
    "    outputs.sort()\n",
    "    return np.mean(outputs), (outputs[int((1.-ci)/2*ns)], outputs[int((1.+ci)/2*ns)])\n",
    "\n",
    "\n",
    "# Internal functions\n",
    "def avgdigamma(points, dvec):\n",
    "    # This part finds number of neighbors in some radius in the marginal space\n",
    "    # returns expectation value of <psi(nx)>\n",
    "    N = len(points)\n",
    "    tree = ss.cKDTree(points)\n",
    "    avg = 0.\n",
    "    for i in range(N):\n",
    "        dist = dvec[i]\n",
    "        # subtlety, we don't include the boundary point,\n",
    "        # but we are implicitly adding 1 to kraskov def bc center point is included\n",
    "        num_points = len(tree.query_ball_point(points[i], dist-1e-15, p=float('inf')))\n",
    "        avg += digamma(num_points)/N\n",
    "    return avg\n",
    "\n",
    "\n",
    "def zip2(*args):\n",
    "    # zip2(x,y) takes the lists of vectors and makes it a list of vectors in a joint space\n",
    "    # E.g. zip2([[1],[2],[3]],[[4],[5],[6]]) = [[1,4],[2,5],[3,6]]\n",
    "    return [sum(sublist, []) for sublist in zip(*args)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def information_gain(f1, f2):\n",
    "    \"\"\"\n",
    "    This function calculates the information gain, where ig(f1,f2) = H(f1) - H(f1|f2)\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    f1: {numpy array}, shape (n_samples,)\n",
    "    f2: {numpy array}, shape (n_samples,)\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    ig: {float}\n",
    "    \"\"\"\n",
    "\n",
    "    ig = entropyd(f1) - conditional_entropy(f1, f2)\n",
    "    return ig\n",
    "\n",
    "\n",
    "def conditional_entropy(f1, f2):\n",
    "    \"\"\"\n",
    "    This function calculates the conditional entropy, where ce = H(f1) - I(f1;f2)\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    f1: {numpy array}, shape (n_samples,)\n",
    "    f2: {numpy array}, shape (n_samples,)\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    ce: {float}\n",
    "        ce is conditional entropy of f1 and f2\n",
    "    \"\"\"\n",
    "\n",
    "    ce = entropyd(f1) - midd(f1, f2)\n",
    "    return ce\n",
    "\n",
    "\n",
    "def su_calculation(f1, f2):\n",
    "    \"\"\"\n",
    "    This function calculates the symmetrical uncertainty, where su(f1,f2) = 2*IG(f1,f2)/(H(f1)+H(f2))\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    f1: {numpy array}, shape (n_samples,)\n",
    "    f2: {numpy array}, shape (n_samples,)\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    su: {float}\n",
    "        su is the symmetrical uncertainty of f1 and f2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate information gain of f1 and f2, t1 = ig(f1,f2)\n",
    "    t1 = information_gain(f1, f2)\n",
    "    # calculate entropy of f1, t2 = H(f1)\n",
    "    t2 = entropyd(f1)\n",
    "    # calculate entropy of f2, t3 = H(f2)\n",
    "    t3 = entropyd(f2)\n",
    "    # su(f1,f2) = 2*t1/(t2+t3)\n",
    "    su = 2.0*t1/(t2+t3)\n",
    "\n",
    "    return su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.bug.values\n",
    "X = df.drop(labels = ['bug'],axis = 1)\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfs(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
