{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import platform\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import SMOTE\n",
    "import feature_selector\n",
    "import DE\n",
    "import CFS\n",
    "import birch\n",
    "import metrics.abcd\n",
    "\n",
    "import metrices\n",
    "import measures\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source1 = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/src/data/1385/exp1/2'\n",
    "if platform.system() == 'Darwin' or platform.system() == 'Linux':\n",
    "    _dir = data_source1 + '/'\n",
    "else:\n",
    "    _dir = data_source1 + '\\\\'\n",
    "\n",
    "clusters = [(join(_dir, f)) for f in listdir(_dir) if Path(join(_dir, f)).is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get cluster wise data for summarzation\n",
    "count = 0\n",
    "count_not = 0\n",
    "count_yes = 0\n",
    "precentage = 0\n",
    "bellwether = {}\n",
    "for cluster in clusters:\n",
    "    df = pd.read_csv(cluster + '/1385_LR_bellwether_f1.csv')\n",
    "    counts = []\n",
    "    for row in range(df.shape[0]):\n",
    "        j = df.iloc[row].values[1:]\n",
    "        value = sum(i >= 0.66 for i in j)\n",
    "        counts.append(value)\n",
    "    df['>=0.6'] = counts\n",
    "    precentage = max(counts)/df.shape[0]\n",
    "    #print(cluster,df.iloc[counts.index(max(counts)),0])\n",
    "    bellwether[cluster.rsplit('/',1)[1]] = df.iloc[counts.index(max(counts)),0]\n",
    "    count += max(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop(labels = ['Host','Vcs','Project','File','PL','IssueTracking'],axis=1)\n",
    "    df = df.dropna()\n",
    "    df = df[['TLOC', 'TNF', 'TNC', 'TND', 'LOC', 'CL', 'NStmt', 'NFunc',\n",
    "       'RCC', 'MNL', 'avg_WMC', 'max_WMC', 'total_WMC', 'avg_DIT', 'max_DIT',\n",
    "       'total_DIT', 'avg_RFC', 'max_RFC', 'total_RFC', 'avg_NOC', 'max_NOC',\n",
    "       'total_NOC', 'avg_CBO', 'max_CBO', 'total_CBO', 'avg_DIT.1',\n",
    "       'max_DIT.1', 'total_DIT.1', 'avg_NIV', 'max_NIV', 'total_NIV',\n",
    "       'avg_NIM', 'max_NIM', 'total_NIM', 'avg_NOM', 'max_NOM', 'total_NOM',\n",
    "       'avg_NPBM', 'max_NPBM', 'total_NPBM', 'avg_NPM', 'max_NPM', 'total_NPM',\n",
    "       'avg_NPRM', 'max_NPRM', 'total_NPRM', 'avg_CC', 'max_CC', 'total_CC',\n",
    "       'avg_FANIN', 'max_FANIN', 'total_FANIN', 'avg_FANOUT', 'max_FANOUT',\n",
    "       'total_FANOUT', 'NRev', 'NFix', 'avg_AddedLOC', 'max_AddedLOC',\n",
    "       'total_AddedLOC', 'avg_DeletedLOC', 'max_DeletedLOC',\n",
    "       'total_DeletedLOC', 'avg_ModifiedLOC', 'max_ModifiedLOC',\n",
    "       'total_ModifiedLOC','Buggy']]\n",
    "    return df\n",
    "\n",
    "def get_features(df):\n",
    "    fs = feature_selector.featureSelector()\n",
    "    df,_feature_nums,features = fs.cfs_bfs(df)\n",
    "    return df,features\n",
    "\n",
    "def apply_cfs(df):\n",
    "    y = df.Buggy.values\n",
    "    X = df.drop(labels = ['Buggy'],axis = 1)\n",
    "    X = X.values\n",
    "    selected_cols = CFS.cfs(X,y)\n",
    "    cols = df.columns[[selected_cols]].tolist()\n",
    "    cols.append('Buggy')\n",
    "    return df[cols],cols\n",
    "    \n",
    "def apply_smote(df):\n",
    "    cols = df.columns\n",
    "    smt = SMOTE.smote(df)\n",
    "    df = smt.run()\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def load_data(path,target):\n",
    "    df = pd.read_csv(path)\n",
    "    if path == 'data/jm1.csv':\n",
    "        df = df[~df.uniq_Op.str.contains(\"\\?\")]\n",
    "    y = df[target]\n",
    "    X = df.drop(labels = target, axis = 1)\n",
    "    X = X.apply(pd.to_numeric)\n",
    "    return X,y\n",
    "\n",
    "# Cluster Driver\n",
    "def cluster_driver(df,print_tree = True):\n",
    "    X = df.apply(pd.to_numeric)\n",
    "    cluster = birch.birch(branching_factor=20)\n",
    "    #X.set_index('Project Name',inplace=True)\n",
    "    cluster.fit(X)\n",
    "    cluster_tree,max_depth = cluster.get_cluster_tree()\n",
    "    #cluster_tree = cluster.model_adder(cluster_tree)\n",
    "    if print_tree:\n",
    "        cluster.show_clutser_tree()\n",
    "    return cluster,cluster_tree,max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cluster_id=0] N_children: 9 N_samples: 697\n",
      "> [cluster_id=1] N_children: 9 N_samples: 87\n",
      "> > [cluster_id=2] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=3] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=4] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=5] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=6] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=7] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=8] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=9] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=10] N_children: 0 N_samples: 4\n",
      "> [cluster_id=11] N_children: 2 N_samples: 4\n",
      "> > [cluster_id=12] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=13] N_children: 0 N_samples: 3\n",
      "> [cluster_id=14] N_children: 10 N_samples: 103\n",
      "> > [cluster_id=15] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=16] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=17] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=18] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=19] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=20] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=21] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=22] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=23] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=24] N_children: 0 N_samples: 18\n",
      "> [cluster_id=25] N_children: 0 N_samples: 2\n",
      "> [cluster_id=26] N_children: 0 N_samples: 4\n",
      "> [cluster_id=27] N_children: 0 N_samples: 1\n",
      "> [cluster_id=28] N_children: 6 N_samples: 64\n",
      "> > [cluster_id=29] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=30] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=31] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=32] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=33] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=34] N_children: 0 N_samples: 7\n",
      "> [cluster_id=35] N_children: 19 N_samples: 222\n",
      "> > [cluster_id=36] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=37] N_children: 0 N_samples: 10\n",
      "> > [cluster_id=38] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=39] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=40] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=41] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=42] N_children: 0 N_samples: 16\n",
      "> > [cluster_id=43] N_children: 0 N_samples: 8\n",
      "> > [cluster_id=44] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=45] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=46] N_children: 0 N_samples: 17\n",
      "> > [cluster_id=47] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=48] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=49] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=50] N_children: 0 N_samples: 12\n",
      "> > [cluster_id=51] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=52] N_children: 0 N_samples: 19\n",
      "> > [cluster_id=53] N_children: 0 N_samples: 4\n",
      "> > [cluster_id=54] N_children: 0 N_samples: 7\n",
      "> [cluster_id=55] N_children: 20 N_samples: 210\n",
      "> > [cluster_id=56] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=57] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=58] N_children: 0 N_samples: 3\n",
      "> > [cluster_id=59] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=60] N_children: 0 N_samples: 9\n",
      "> > [cluster_id=61] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=62] N_children: 0 N_samples: 15\n",
      "> > [cluster_id=63] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=64] N_children: 0 N_samples: 5\n",
      "> > [cluster_id=65] N_children: 0 N_samples: 13\n",
      "> > [cluster_id=66] N_children: 0 N_samples: 6\n",
      "> > [cluster_id=67] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=68] N_children: 0 N_samples: 2\n",
      "> > [cluster_id=69] N_children: 0 N_samples: 1\n",
      "> > [cluster_id=70] N_children: 0 N_samples: 11\n",
      "> > [cluster_id=71] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=72] N_children: 0 N_samples: 18\n",
      "> > [cluster_id=73] N_children: 0 N_samples: 20\n",
      "> > [cluster_id=74] N_children: 0 N_samples: 7\n",
      "> > [cluster_id=75] N_children: 0 N_samples: 9\n"
     ]
    }
   ],
   "source": [
    "attr_dict = pd.read_pickle('data/1385/projects/selected_attr.pkl')\n",
    "attr_df = pd.DataFrame.from_dict(attr_dict,orient='index')\n",
    "cluster,cluster_tree,max_depth = cluster_driver(attr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_attr_dict = pd.read_pickle('data/1385/projects/other_selected_attr.pkl')\n",
    "_attr_df = pd.DataFrame.from_dict(_attr_dict,orient='index')\n",
    "test_projects = _attr_df.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_cluster = cluster.predict(_attr_df,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guineu.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.12      0.21        58\n",
      "        True       0.32      0.92      0.48        26\n",
      "\n",
      "    accuracy                           0.37        84\n",
      "   macro avg       0.55      0.52      0.34        84\n",
      "weighted avg       0.64      0.37      0.29        84\n",
      "\n",
      "campsoft.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00       104\n",
      "        True       0.01      1.00      0.02         1\n",
      "\n",
      "    accuracy                           0.01       105\n",
      "   macro avg       0.00      0.50      0.01       105\n",
      "weighted avg       0.00      0.01      0.00       105\n",
      "\n",
      "gtad.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00        71\n",
      "        True       0.03      1.00      0.05         2\n",
      "\n",
      "    accuracy                           0.03        73\n",
      "   macro avg       0.01      0.50      0.03        73\n",
      "weighted avg       0.00      0.03      0.00        73\n",
      "\n",
      "rhex.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.63      0.75        81\n",
      "        True       0.21      0.67      0.32        12\n",
      "\n",
      "    accuracy                           0.63        93\n",
      "   macro avg       0.57      0.65      0.53        93\n",
      "weighted avg       0.83      0.63      0.69        93\n",
      "\n",
      "owlib.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.42      0.03      0.05       171\n",
      "        True       0.39      0.94      0.56       115\n",
      "\n",
      "    accuracy                           0.40       286\n",
      "   macro avg       0.41      0.48      0.30       286\n",
      "weighted avg       0.41      0.40      0.26       286\n",
      "\n",
      "ktc.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.49      0.54       170\n",
      "        True       0.42      0.54      0.47       115\n",
      "\n",
      "    accuracy                           0.51       285\n",
      "   macro avg       0.51      0.51      0.51       285\n",
      "weighted avg       0.53      0.51      0.51       285\n",
      "\n",
      "jedit.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.57      0.57      0.57        75\n",
      "        True       0.48      0.48      0.48        61\n",
      "\n",
      "    accuracy                           0.53       136\n",
      "   macro avg       0.52      0.52      0.52       136\n",
      "weighted avg       0.53      0.53      0.53       136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(predicted_cluster)):\n",
    "    c_id = predicted_cluster[i]\n",
    "    s_project = bellwether[str(c_id)]\n",
    "    s_path = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted/' + s_project\n",
    "    df = prepare_data(s_path)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    d = {'buggy': True, 'clean': False}\n",
    "    df['Buggy'] = df['Buggy'].map(d)\n",
    "    df, s_cols = apply_cfs(df)\n",
    "    df = apply_smote(df)\n",
    "    y = df.Buggy\n",
    "    X = df.drop(labels = ['Buggy'],axis = 1)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X,y)\n",
    "    \n",
    "    d_project = test_projects[i]\n",
    "    print(d_project)\n",
    "    d_path = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted/' + d_project\n",
    "    _test_df = prepare_data(d_path)\n",
    "    _df_test_loc = _test_df.LOC\n",
    "    test_df = _test_df[s_cols]\n",
    "    test_df.reset_index(drop=True,inplace=True)\n",
    "    d = {'buggy': True, 'clean': False}\n",
    "    test_df['Buggy'] = test_df['Buggy'].map(d)\n",
    "    test_y = test_df.Buggy\n",
    "    test_X = test_df.drop(labels = ['Buggy'],axis = 1)\n",
    "    predicted = clf.predict(test_X)\n",
    "    print(classification_report(test_y,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
