{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import platform\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import traceback\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import SMOTE\n",
    "import feature_selector\n",
    "import DE\n",
    "import CFS\n",
    "import metrics.abcd\n",
    "\n",
    "import metrices\n",
    "import measures\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source1 = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted'\n",
    "if platform.system() == 'Darwin' or platform.system() == 'Linux':\n",
    "    _dir = data_source1 + '/'\n",
    "else:\n",
    "    _dir = data_source1 + '\\\\'\n",
    "projects = [f for f in listdir(_dir) if isfile(join(_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop(labels = ['Host','Vcs','Project','File','PL','IssueTracking'],axis=1)\n",
    "    df = df.dropna()\n",
    "    df = df[['TLOC', 'TNF', 'TNC', 'TND', 'LOC', 'CL', 'NStmt', 'NFunc',\n",
    "       'RCC', 'MNL', 'avg_WMC', 'max_WMC', 'total_WMC', 'avg_DIT', 'max_DIT',\n",
    "       'total_DIT', 'avg_RFC', 'max_RFC', 'total_RFC', 'avg_NOC', 'max_NOC',\n",
    "       'total_NOC', 'avg_CBO', 'max_CBO', 'total_CBO', 'avg_DIT.1',\n",
    "       'max_DIT.1', 'total_DIT.1', 'avg_NIV', 'max_NIV', 'total_NIV',\n",
    "       'avg_NIM', 'max_NIM', 'total_NIM', 'avg_NOM', 'max_NOM', 'total_NOM',\n",
    "       'avg_NPBM', 'max_NPBM', 'total_NPBM', 'avg_NPM', 'max_NPM', 'total_NPM',\n",
    "       'avg_NPRM', 'max_NPRM', 'total_NPRM', 'avg_CC', 'max_CC', 'total_CC',\n",
    "       'avg_FANIN', 'max_FANIN', 'total_FANIN', 'avg_FANOUT', 'max_FANOUT',\n",
    "       'total_FANOUT', 'NRev', 'NFix', 'avg_AddedLOC', 'max_AddedLOC',\n",
    "       'total_AddedLOC', 'avg_DeletedLOC', 'max_DeletedLOC',\n",
    "       'total_DeletedLOC', 'avg_ModifiedLOC', 'max_ModifiedLOC',\n",
    "       'total_ModifiedLOC','Buggy']]\n",
    "    return df\n",
    "\n",
    "def get_features(df):\n",
    "    fs = feature_selector.featureSelector()\n",
    "    df,_feature_nums,features = fs.cfs_bfs(df)\n",
    "    return df,features\n",
    "\n",
    "def apply_cfs(df):\n",
    "    y = df.Buggy.values\n",
    "    X = df.drop(labels = ['Buggy'],axis = 1)\n",
    "    X = X.values\n",
    "    selected_cols = CFS.cfs(X,y)\n",
    "    cols = df.columns[[selected_cols]].tolist()\n",
    "    cols.append('Buggy')\n",
    "    return df[cols],cols\n",
    "    \n",
    "def apply_smote(df):\n",
    "    cols = df.columns\n",
    "    smt = SMOTE.smote(df)\n",
    "    df = smt.run()\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "def tune_learner(learner, train_X, train_Y, tune_X, tune_Y, goal,loc=None,target_class=None):\n",
    "    if not target_class:\n",
    "        target_class = goal\n",
    "    clf = learner(train_X, train_Y, tune_X, tune_Y, goal,loc)\n",
    "    tuner = DE.DE_Tune_ML(clf, clf.get_param(), goal, target_class)\n",
    "    return tuner.Tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = {}\n",
    "count = 0\n",
    "for project in projects:\n",
    "    try:\n",
    "        path = '/Users/suvodeepmajumder/Documents/AI4SE/bellwether_comminity/data/1385/converted/' + project\n",
    "        print(project)\n",
    "        df = prepare_data(path)\n",
    "        if df.shape[0] < 50:\n",
    "            continue\n",
    "        else:\n",
    "            count+=1\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        d = {'buggy': True, 'clean': False}\n",
    "        df['Buggy'] = df['Buggy'].map(d)\n",
    "        buggy = df[df['Buggy'] == True]\n",
    "        buggy_percentage = buggy.shape[0]/df.shape[0]\n",
    "        y = df.Buggy\n",
    "        X = df.drop(labels = ['Buggy'],axis = 1)\n",
    "        kf = StratifiedKFold(n_splits = 5)\n",
    "        goal = 'f1'\n",
    "        learner = [SK_LR][0]\n",
    "        F = {}\n",
    "        score = {}\n",
    "        for i in range(5):\n",
    "            for train_index, tune_index in kf.split(X, y):\n",
    "                X_train, X_tune = X.iloc[train_index], X.iloc[tune_index]\n",
    "                y_train, y_tune = y[train_index], y[tune_index]\n",
    "                _df_tune_loc = X_tune.LOC\n",
    "                #clf = LogisticRegression()\n",
    "                clf = SVC()\n",
    "                clf.fit(X_train,y_train)\n",
    "                predicted = clf.predict(X_tune)\n",
    "                abcd = metrices.measures(y_tune,predicted,_df_tune_loc)\n",
    "                F['f1'] = [abcd.calculate_f1_score()]\n",
    "                F['precision'] = [abcd.calculate_precision()]\n",
    "                F['recall'] = [abcd.calculate_recall()]\n",
    "                F['g-score'] = [abcd.get_g_score()]\n",
    "                F['d2h'] = [abcd.calculate_d2h()]\n",
    "                F['pci_20'] = [abcd.get_pci_20()]\n",
    "                F['ifa'] = [abcd.get_ifa()]\n",
    "                F['pd'] = [abcd.get_pd()]\n",
    "                F['pf'] = [abcd.get_pf()]\n",
    "                _F = copy.deepcopy(F)\n",
    "                #print(_F)\n",
    "                if 'f1' not in score.keys():\n",
    "                    _F['buggy_prec'] = [buggy_percentage]\n",
    "                    score = _F\n",
    "                else:\n",
    "                    score['f1'].append(F['f1'][0])\n",
    "                    score['precision'].append(F['precision'][0])\n",
    "                    score['recall'].append(F['recall'][0])\n",
    "                    score['g-score'].append(F['g-score'][0])\n",
    "                    score['d2h'].append(F['d2h'][0])\n",
    "                    score['pci_20'].append(F['pci_20'][0])\n",
    "                    score['ifa'].append(F['ifa'][0])\n",
    "                    score['pd'].append(F['pd'][0])\n",
    "                    score['pf'].append(F['pf'][0])\n",
    "                    score['buggy_prec'].append(buggy_percentage)\n",
    "            final_score[project] = score \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/1385/1385_LR_default_svm.pkl', 'wb') as handle:\n",
    "    pickle.dump(final_score, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/1385/1385_LR_default_svm.pkl')\n",
    "results = []\n",
    "for project in df.keys():\n",
    "    results.append([project,np.median(df[project]['f1']),\n",
    "                   np.median(df[project]['precision']),\n",
    "                   np.median(df[project]['recall']),\n",
    "                   np.median(df[project]['g-score']),\n",
    "                   np.median(df[project]['d2h']),\n",
    "                   np.median(df[project]['pci_20']),\n",
    "                   np.median(df[project]['ifa']),\n",
    "                   np.median(df[project]['pd']),\n",
    "                   np.median(df[project]['pf']),\n",
    "                   np.median(df[project]['buggy_prec'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for project in df.keys():\n",
    "    results.append([project,np.median(df[project]['f1']),\n",
    "                   np.median(df[project]['precision']),\n",
    "                   np.median(df[project]['recall']),\n",
    "                   np.median(df[project]['g-score']),\n",
    "                   np.median(df[project]['d2h']),\n",
    "                   np.median(df[project]['pci_20']),\n",
    "                   np.median(df[project]['ifa']),\n",
    "                   np.median(df[project]['pd']),\n",
    "                   np.median(df[project]['pf']),\n",
    "                   np.median(df[project]['buggy_prec'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results,columns = ['project','f1','precision','recall','g-score','d2h','pci_20','ifa','pd','pf','buggyness'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('data/1385/1385_LR_default_svm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.array([.68,.54,.77,.66,.66,.73,.5,.77,.62,.58,.7,.46,.73,.66,.66,.76,.54,.56,.66,.66,.71,.29,.69,.58,.54])\n",
    "print(np.median(score))\n",
    "plt.boxplot(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
